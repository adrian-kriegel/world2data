World2Data turns ordinary 2D video into structured 3D world data that teams can inspect, reason over, and export to OpenUSD. We focus on a practical bottleneck in robotics and simulation: converting raw video into reusable world representations still often requires manual setup, custom scripts, or expensive sensors.

In this prototype, one video run produces keyframes, camera trajectory, dense 3D reconstruction (MASt3R), semantic detections, temporal object/state traces, and interoperable outputs (`.usda`, `.rrd`, `.ply`, and scene-graph JSON). Recent overnight output demonstrates full-span processing of a ~33s clip with ~120 temporal frames and multi-million-point reconstruction, not just a short snippet.

The system is engineered for demo reliability: deterministic controls, cached reconstruction artifacts, and graceful degradation when external model services are constrained. We also defined an OpenUSD Layering Protocol that moves collaboration from monolithic file overwrite to deterministic layer composition with provenance and namespace ownership.

This makes World2Data immediately useful as a bridge from raw video to simulation-ready assets, while providing a clear path to production hardening through layered USD assembly, validation checks, and human QA override layers.
