World2Data turns ordinary 2D video into a structured 3D world model that machines can reason over and export to OpenUSD. The problem we target is that robotics, simulation, and digital-twin workflows still require expensive sensors or manual scene annotation before teams can run meaningful experiments. During this hackathon, we built an automated pipeline that extracts keyframes, reconstructs 3D geometry with MASt3R, attaches semantic object understanding, and exports results as interoperable USD assets plus a temporal visualization in Rerun.

Our solution is designed for teams that need fast environment grounding: robotics developers, simulation engineers, and AI product teams building real-world agents. Today, the system produces camera trajectories, dense point clouds, object metadata, state annotations, and review-ready scene graphs from a single video input. It already supports robust fallback behavior when individual model services are unavailable, so demos can still run under constrained API conditions.

The strongest outcome is practical end-to-end functionality: from one input video we generate reproducible 3D artifacts and a clear audit trail of what was detected, how objects are tracked over time, and where confidence is high or low. This makes World2Data immediately valuable as a bridge between raw visual data and production-ready 3D/physics pipelines.
